# Snake AI (PyTorch, DQN)

## –û–ø–∏—Å–∞–Ω–∏–µ(description)

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏–≥—Ä–∞—Ç—å –≤ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫—É—é –∏–≥—Ä—É ¬´–ó–º–µ–π–∫–∞¬ª —Å –ø–æ–º–æ—â—å—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ Deep Q-Learning (DQN).  
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ PyTorch –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –∞ —Ç–∞–∫–∂–µ pygame –¥–ª—è –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.

This project implements training a neural network to play the classic Snake game using the Deep Q-Learning (DQN) algorithm.  
It uses PyTorch for building and training the model, and pygame for the graphical interface.

### üöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ –∏–≥—Ä–∞—Ç—å –≤ –∑–º–µ–π–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Œµ-–∂–∞–¥–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏.
- –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–π–Ω—Ç–æ–≤ (–≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏).
- GUI-—Ä–µ–∂–∏–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –∏–≥—Ä—ã –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è.

### üöÄ Features
- Train an agent to play Snake using the Œµ-greedy strategy.
- Save and load checkpoints (model weights, optimizer state, training statistics).
- GUI mode to demonstrate the trained agent.
- Training logs for performance tracking.

### üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞(Project Structure)
- `snake.py` ‚Äî –ª–æ–≥–∏–∫–∞ –∏–≥—Ä—ã(game logic).
- `model.py` ‚Äî –∞–≥–µ–Ω—Ç, –Ω–µ–π—Ä–æ—Å–µ—Ç—å, –æ–±—É—á–µ–Ω–∏–µ(agent, neural network, training loop).
- `gui.py` ‚Äî –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∏–≥—Ä—ã —Å –ò–ò(graphical interface with AI).
- `checkpoint.pth` ‚Äî —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è(saved training progress).
- `training_log.csv` ‚Äî –∂—É—Ä–Ω–∞–ª –æ–±—É—á–µ–Ω–∏—è(training log).

### ‚öôÔ∏è –£—Å—Ç–∞–Ω–æ–≤–∫–∞(Installation)
```bash
git clone <—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π>
cd snake-ai
pip install -r requirements.txt
